{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ad6d4a",
   "metadata": {},
   "source": [
    "- Install Dependencies and Export  gemini api key\n",
    "- Get gemini api key from [Google AI Studio](https://aistudio.google.com/app/api-keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54fdad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -q -U google-genai\n",
    "\n",
    "!export GEMINI_API_KEY = <Gemini-API-KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36758416",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee95be08",
   "metadata": {},
   "source": [
    "## 1.1 LLM Inference\n",
    "![LLM-Inference](./images/1.1-LLM-Inference.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c341b4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Some must-see places in Pokhara include Phewa Lake, where you can enjoy boating and lakeside views, and the World Peace Pagoda, offering panoramic views of the Himalayas and the city. Don't miss Devi's Fall, a unique waterfall, and the serene Begnas Lake for a peaceful retreat.\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "config = types.GenerateContentConfig(\n",
    "    system_instruction=\"You are a helpful, knowledgeable assistant that can answer any question. Answer any question the user asks in 1-2 sentences.\"\n",
    ")\n",
    "\n",
    "def respond(history, config=config) -> str:\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=history,\n",
    "        config = config\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "respond(\"What are the must-see places in Pokhara?\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1115deb5",
   "metadata": {},
   "source": [
    "## 1.2 The Loop (Birth of Chatbot)\n",
    "![LLM-Inference](./images/1.2-The-Loop.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b974af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What are the must-see places in Pokhara?\n",
      "AI: Some must-see places in Pokhara include Phewa Lake, World Peace Pagoda, and Sarangkot. You should also consider visiting Devi's Fall, Gupteshwor Cave, and the International Mountain Museum.\n",
      "\n",
      "User: I prefer peaceful spots - can you adjust the plan?\n",
      "AI: I can definitely adjust the plan to prioritize peaceful spots. To do this best, could you tell me what kind of activities you enjoy in peaceful settings and where you're planning to go?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = input()\n",
    "    print(\"User:\", query)\n",
    "    \n",
    "    if not query or query == \"exit\":\n",
    "        break\n",
    "\n",
    "    print(\"AI:\", respond(query).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49add18b",
   "metadata": {},
   "source": [
    "- It only have current query so it cannot modify the previous response properly. Lets give it memory in context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2d63a4",
   "metadata": {},
   "source": [
    "## 1.3 Conversation History (Memory Emerges)\n",
    "![](images/1.3-History.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305ca42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  What are the must-see places in Pokhara?\n",
      "LLM: Some must-see places in Pokhara include Phewa Lake, the World Peace Pagoda, and the Begnas Lake. You might also enjoy Devi's Fall and the Gupteshwor Cave.\n",
      "\n",
      "\n",
      "User:  I prefer peaceful spots — can you adjust the plan?\n",
      "LLM: For peaceful spots in Pokhara, consider visiting the World Peace Pagoda for panoramic views and serenity, or head to the less crowded Begnas Lake for a tranquil boating experience. You might also enjoy exploring the serene monasteries and temples around the valley.\n",
      "\n",
      "\n",
      "User:  What is the weather in Pokhara right now?\n",
      "LLM: Unfortunately, I do not have real-time access to weather information. You can check a reliable weather app or website for the current weather conditions in Pokhara.\n",
      "\n",
      "\n",
      "User:  \n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "    \n",
    "while True:\n",
    "    query = input()\n",
    "    print(\"User: \", query)\n",
    "    \n",
    "    if query.lower() in ['exit', 'quit', '']:\n",
    "        break\n",
    "    \n",
    "    # Add user query to history\n",
    "    history.append({\"role\": \"user\", \"parts\": [{\"text\": query}]})\n",
    "    \n",
    "    # Get response\n",
    "    response = respond(history).text\n",
    "    print(f\"LLM: {response}\\n\")\n",
    "    \n",
    "    # Add model response to history\n",
    "    history.append({\"role\": \"model\", \"parts\": [{\"text\": response}]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c7f23",
   "metadata": {},
   "source": [
    "- we can see it does not have information about the weather."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2ed5d5",
   "metadata": {},
   "source": [
    "## 1.4 Tools (Agency Appears)\n",
    "- Lets write a function that takes `place_name` and returns a random weather\n",
    "\n",
    "![](images/1.4-Tool-Use.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff105291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import random\n",
    "\n",
    "client = genai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17485e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking weather for 'Pokhara'...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'location': 'Pokhara',\n",
       " 'temp_max': '31',\n",
       " 'temp_min': '21',\n",
       " 'conditions': 'Thunderstorm'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Tool Implementation\n",
    "def get_weather(place_name):\n",
    "    \"\"\"Return dummy random weather for the given place_name.\"\"\"\n",
    "    \n",
    "    print(f\"Checking weather for '{place_name}'...\")\n",
    "\n",
    "    conditions_list = [\"Sunny\", \"Cloudy\", \"Rainy\", \"Thunderstorm\", \"Snowy\", \"Windy\", \"Foggy\"]\n",
    "    \n",
    "    temp_min = random.randint(10, 25)       # minimum temperature\n",
    "    temp_max = random.randint(temp_min, temp_min + 15)  # max is at least min\n",
    "    \n",
    "    return {\n",
    "        \"location\": place_name,\n",
    "        \"temp_max\": str(temp_max),\n",
    "        \"temp_min\": str(temp_min),\n",
    "        \"conditions\": random.choice(conditions_list)\n",
    "    }\n",
    "\n",
    "get_weather(\"Pokhara\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c94b45",
   "metadata": {},
   "source": [
    "- Lets give this function as tool to our LLM\n",
    "- refer to [gemini-api-docs](https://ai.google.dev/gemini-api/docs/function-calling) for more information on function calling\n",
    "\n",
    "- Below code is modified version of template given in [gemini-api-docs](https://ai.google.dev/gemini-api/docs/function-calling) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42ac3e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tool definitions\n",
    "weather_function = {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Gets weather forecast for a specified location\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"place_name\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Name of the city or location\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"place_name\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = types.Tool(function_declarations=[weather_function])\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[tools],\n",
    "    system_instruction=\"You are a general-purpose helpful assistant. Answer any question the user asks in 1-2 short sentences. You have a weather tool available - use it ONLY when users specifically ask about weather/temperature/forecast. For all other questions, answer directly without using tools.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3353f0f9",
   "metadata": {},
   "source": [
    "- And lets add another function to generate response with tool use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8dc78b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Giving tool to LLM\n",
    "def respond(history, config):\n",
    "    # print(f\"\\n\\n history:{history}\")\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=history,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    candidate = response.candidates[0]\n",
    "    if not candidate.content or not candidate.content.parts:\n",
    "        return \"I couldn't generate a response.\"\n",
    "    \n",
    "    first_part = candidate.content.parts[0]\n",
    "    \n",
    "    # Check if it's a function call\n",
    "    if hasattr(first_part, 'function_call') and first_part.function_call:\n",
    "        function_call = first_part.function_call\n",
    "        \n",
    "        if function_call.name == \"get_weather\":\n",
    "            result = get_weather(**function_call.args)\n",
    "            \n",
    "            # Add function call results to history\n",
    "            history.append({\"role\": \"model\", \"parts\": [{\"function_call\": function_call}]})\n",
    "            history.append({\"role\": \"user\", \"parts\": [{\"function_response\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"response\": result\n",
    "            }}]})\n",
    "            \n",
    "            # Re-invokek the LLM\n",
    "            # print(f\"\\n\\n history:{history}\")\n",
    "            final_response = client.models.generate_content(\n",
    "                model=\"gemini-2.5-flash\",\n",
    "                contents=history,\n",
    "                config=config\n",
    "            )\n",
    "            \n",
    "            if final_response.candidates and final_response.candidates[0].content.parts:\n",
    "                return final_response.text\n",
    "            return \"I couldn't generate a response with the weather data.\"\n",
    "    \n",
    "    # Check if it's text\n",
    "    if hasattr(first_part, 'text'):\n",
    "        return response.text\n",
    "    \n",
    "    return \"I couldn't understand the response.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6fe1744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  What are the must-see places in Pokhara?\n",
      "LLM: Pokhara is known for its stunning natural beauty. Some must-see places include Phewa Lake, Sarangkot, Davis Falls, and the World Peace Pagoda.\n",
      "\n",
      "User:  I prefer peaceful spots — can you adjust the plan?\n",
      "LLM: For a more peaceful experience in Pokhara, consider spending more time around Phewa Lake, visiting the World Peace Pagoda for serene views, and exploring the less crowded areas of Begnas Lake. You might also enjoy a quiet boat ride on Phewa Lake early in the morning.\n",
      "\n",
      "User:  What is the weather in Pokhara right now?\n",
      "Checking weather for 'Pokhara'...\n",
      "LLM: The weather in Pokhara right now is rainy, with temperatures ranging from 14 to 15 degrees Celsius.\n",
      "\n",
      "User:  \n"
     ]
    }
   ],
   "source": [
    "# 4. The Loop\n",
    "history = []\n",
    "\n",
    "while True:\n",
    "    query = input()\n",
    "    print(\"User: \", query)\n",
    "    \n",
    "    if query.lower() in ['exit', 'quit', '']:\n",
    "        break\n",
    "    \n",
    "    # Add user query to history\n",
    "    history.append({\"role\": \"user\", \"parts\": [{\"text\": query}]})\n",
    "    \n",
    "    # Get response\n",
    "    response = respond(history, config)\n",
    "    print(f\"LLM: {response}\\n\")\n",
    "    \n",
    "    # Add model response to history\n",
    "    history.append({\"role\": \"model\", \"parts\": [{\"text\": response}]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd20ee",
   "metadata": {},
   "source": [
    "- **Exercise:** Replace get_weather function with below code and see if it gets actual weather\n",
    "\n",
    "```\n",
    "import requests\n",
    "\n",
    "def get_weather(place_name):\n",
    "    \"\"\"Get weather for given place_name.\"\"\"\n",
    "    \n",
    "    print(\"Checking weather...\")\n",
    "\n",
    "    response = requests.get(f\"https://wttr.in/{place_name}?format=j1\")\n",
    "    data = response.json()\n",
    "    tomorrow = data[\"weather\"][1]\n",
    "    \n",
    "    return {\n",
    "        \"location\": place_name,\n",
    "        \"temp_max\": tomorrow[\"maxtempC\"],\n",
    "        \"temp_min\": tomorrow[\"mintempC\"],\n",
    "        \"conditions\": tomorrow[\"hourly\"][0][\"weatherDesc\"][0][\"value\"]\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70079b5a",
   "metadata": {},
   "source": [
    "# 2. Coding Agent: A Minimal Claude-like CLI\n",
    "\n",
    "- In this section, we will:\n",
    "  1.  implement tools to: read, write, list files\n",
    "  2. Define tools for gemini (as instructed in [gemini-api-docs](https://ai.google.dev/gemini-api/docs/function-calling) page.)\n",
    "  3. Write `respond` function that can call the tool call (similar to last section)\n",
    "  4. Put it in loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "045317e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a45618a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.1 Tool implementations\n",
    "def read_file(path, offset=0, limit=None):\n",
    "    \"\"\"Read file with line numbers.\"\"\"\n",
    "    \n",
    "    print(\"Read file:  {path}, offset: {} weather...\")\n",
    "\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    if limit is None:\n",
    "        limit = len(lines)\n",
    "    \n",
    "    selected = lines[offset : offset + limit]\n",
    "    return \"\".join(f\"{offset + idx + 1:4}| {line}\" for idx, line in enumerate(selected))\n",
    "\n",
    "\n",
    "def write_file(path, content):\n",
    "    \"\"\"Write content to file.\"\"\"\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(content)\n",
    "    return \"ok\"\n",
    "\n",
    "\n",
    "def list_directory(path):\n",
    "    \"\"\"List files and directories.\"\"\"\n",
    "    items = os.listdir(path)\n",
    "    result = []\n",
    "    for item in sorted(items):\n",
    "        full_path = os.path.join(path, item)\n",
    "        if os.path.isdir(full_path):\n",
    "            result.append(f\"[DIR]  {item}\")\n",
    "        else:\n",
    "            result.append(f\"[FILE] {item}\")\n",
    "    return \"\\n\".join(result)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5185a112",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.2 Tool definitions\n",
    "read_function = {\n",
    "    \"name\": \"read_file\",\n",
    "    \"description\": \"Read file with line numbers (file path, not directory)\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"path\": {\"type\": \"string\", \"description\": \"File path to read\"},\n",
    "            \"offset\": {\"type\": \"number\", \"description\": \"Starting line number (0-indexed)\"},\n",
    "            \"limit\": {\"type\": \"number\", \"description\": \"Number of lines to read\"}\n",
    "        },\n",
    "        \"required\": [\"path\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "write_function = {\n",
    "    \"name\": \"write_file\",\n",
    "    \"description\": \"Write content to file\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"path\": {\"type\": \"string\", \"description\": \"File path to write\"},\n",
    "            \"content\": {\"type\": \"string\", \"description\": \"Content to write\"}\n",
    "        },\n",
    "        \"required\": [\"path\", \"content\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "list_function = {\n",
    "    \"name\": \"list_directory\",\n",
    "    \"description\": \"List files and directories at path\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"path\": {\"type\": \"string\", \"description\": \"Directory path to list\"}\n",
    "        },\n",
    "        \"required\": [\"path\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = types.Tool(function_declarations=[read_function, write_function, list_function])\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[tools],\n",
    "    system_instruction=f\"Concise coding assistant. cwd: {os.getcwd()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530f54fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.3 Respond function (that can call the tool)\n",
    "def respond(history, config):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=history,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    if not response.candidates:\n",
    "        return \"I couldn't generate a response.\"\n",
    "    \n",
    "    candidate = response.candidates[0]\n",
    "    if not candidate.content or not candidate.content.parts:\n",
    "        return \"I couldn't generate a response.\"\n",
    "    \n",
    "    first_part = candidate.content.parts[0]\n",
    "    \n",
    "    # Check if it's a function call\n",
    "    if hasattr(first_part, 'function_call') and first_part.function_call:\n",
    "        function_call = first_part.function_call\n",
    "        \n",
    "        # Execute the appropriate function\n",
    "        try:\n",
    "            if function_call.name == \"read_file\":\n",
    "                result = read_file(**function_call.args)\n",
    "            elif function_call.name == \"write_file\":\n",
    "                result = write_file(**function_call.args)\n",
    "            elif function_call.name == \"list_directory\":\n",
    "                result = list_directory(**function_call.args)\n",
    "            else:\n",
    "                result = f\"Unknown function: {function_call.name}\"\n",
    "        except Exception as err:\n",
    "            result = f\"error: {err}\"\n",
    "        \n",
    "        # Add function call and result to history\n",
    "        history.append({\"role\": \"model\", \"parts\": [{\"function_call\": function_call}]})\n",
    "        history.append({\"role\": \"user\", \"parts\": [{\"function_response\": {\n",
    "            \"name\": function_call.name,\n",
    "            \"response\": {\"result\": result}\n",
    "        }}]})\n",
    "        \n",
    "        # Get final response with the tool result\n",
    "        final_response = client.models.generate_content(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            contents=history,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        if final_response.candidates and final_response.candidates[0].content.parts:\n",
    "            return final_response.text\n",
    "        return \"I couldn't generate a response with the tool result.\"\n",
    "    \n",
    "    # Check if it's text\n",
    "    if hasattr(first_part, 'text'):\n",
    "        return response.text\n",
    "    \n",
    "    return \"I couldn't understand the response.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cceceef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: create file add.py with function to add two numbers\n",
      "LLM: File add.py created with function to add two numbers.\n",
      "\n",
      "User: create file substract.py with function to substract two numbers\n",
      "LLM: File substract.py created with function to substract two numbers.\n",
      "\n",
      "User: create file math.py that imports these two functions and perform demo calculation\n",
      "LLM: File math.py created. It imports `add` and `substract` functions and performs demo calculations, printing the results.\n",
      "\n",
      "User: \n"
     ]
    }
   ],
   "source": [
    "## 2.4 The Loop\n",
    "history = []\n",
    "    \n",
    "while True:\n",
    "    query = input()\n",
    "    print(f\"User: {query}\")\n",
    "    if query.lower() in ['exit', 'quit', '']:\n",
    "        break\n",
    "    \n",
    "    # Add user query to history\n",
    "    history.append({\"role\": \"user\", \"parts\": [{\"text\": query}]})\n",
    "    \n",
    "    # Get response\n",
    "    response = respond(history, config)\n",
    "    print(f\"LLM: {response}\\n\")\n",
    "    \n",
    "    # Add model response to history\n",
    "    history.append({\"role\": \"model\", \"parts\": [{\"text\": response}]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53646722",
   "metadata": {},
   "source": [
    "- Lets see what it has created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "96274db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def add(a, b):\n",
      "    return a + b"
     ]
    }
   ],
   "source": [
    "!cat add.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bb4280fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def substract(a, b):\n",
      "    return a - b"
     ]
    }
   ],
   "source": [
    "!cat substract.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f28ea47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from add import add\n",
      "from substract import substract\n",
      "\n",
      "result_add = add(5, 3)\n",
      "print(f\"5 + 3 = {result_add}\")\n",
      "\n",
      "result_substract = substract(10, 4)\n",
      "print(f\"10 - 4 = {result_substract}\")"
     ]
    }
   ],
   "source": [
    "!cat math.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "093df2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 + 3 = 8\n",
      "10 - 4 = 6\n"
     ]
    }
   ],
   "source": [
    "# Executing math.py\n",
    "!python3 math.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd058d5",
   "metadata": {},
   "source": [
    "- Great, We have our own coding agent.\n",
    "- We can improve it further with prompt engineering and adding more tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf08474",
   "metadata": {},
   "source": [
    "## 3. Autonomous Personal Assistant: Minimal `openClaw` Implementation\n",
    "- **Minimal openClaw Implementation - from Scratch**\n",
    "\n",
    "![](images/3-Personal-Assistant.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832c1395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ab13e6",
   "metadata": {},
   "source": [
    "### 3.1 SKILLS.md: Define What the Agent Can Do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcc600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile SKILLS.md\n",
    "# SKILLS\n",
    "\n",
    "## Message User in telegram\n",
    "\n",
    "1. Get Telegram token and user_id\n",
    "use run_bash_command to get TELEGRAM_BOT_TOKEN, TELEGRAM_USER_ID from .env file, use bash command: `cat .env` to read them from .env file\n",
    "\n",
    "2. Sending message\n",
    "send a post request to endpoint: \"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage\" with payload like below\n",
    "    \n",
    "```\n",
    "{\n",
    "    'chat_id': TELEGRAM_USER_ID,\n",
    "    'text': <message>\n",
    "}\n",
    "```\n",
    "\n",
    "note: Replace TELEGRAM_BOT_TOKEN, TELEGRAM_USER_ID in above template with actual user token and ids from .env\n",
    "\n",
    "\n",
    "## Checking new research papers on LLM\n",
    "send a GET request to `export.arxiv.org` e.g.\n",
    "https://export.arxiv.org/api/query?search_query=all:llm&start=0&max_results=3&sortBy=submittedDate&sortOrder=descending\n",
    "\n",
    "\n",
    "\n",
    "## Tasks\n",
    "Tasks file: TASKS.md\n",
    "when user asks to update tasks: append to TASKS.md using bash command `echo \"Task\" >> TASKS.md`\n",
    "to read from tasks: use: `cat TASKS.md`\n",
    "Send result of task execution to telegram, without asking for user confirmation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11264a57",
   "metadata": {},
   "source": [
    "### 3.2 Tool: `run_bash_command`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4798cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool implementation\n",
    "def run_bash_command(command):\n",
    "    \"\"\"Execute a bash command and return output.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            command,\n",
    "            shell=True,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30\n",
    "        )\n",
    "        output = result.stdout if result.stdout else result.stderr\n",
    "        return output if output else \"Command executed successfully (no output)\"\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Error: Command timed out after 30 seconds\"\n",
    "    except Exception as err:\n",
    "        return f\"Error: {err}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcddf7c",
   "metadata": {},
   "source": [
    "- Tool Defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed233efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tool definition ---\n",
    "bash_function = {\n",
    "    \"name\": \"run_bash_command\",\n",
    "    \"description\": \"Execute a bash command and return its output\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "            \"command\": {\"type\": \"string\", \"description\": \"Bash command to execute\"}\n",
    "        },\n",
    "        \"required\": [\"command\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = types.Tool(function_declarations=[bash_function])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba1bc46",
   "metadata": {},
   "source": [
    "### ### 3.3 Respond Function with Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02380b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = genai.Client()\n",
    "\n",
    "def respond(history, config):\n",
    "    max_iterations = 10  # Prevent infinite loops\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.5-pro\",\n",
    "            contents=history,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        if not response.candidates:\n",
    "            return \"I couldn't generate a response.\"\n",
    "        \n",
    "        candidate = response.candidates[0]\n",
    "        if not candidate.content or not candidate.content.parts:\n",
    "            return \"I couldn't generate a response.\"\n",
    "        \n",
    "        first_part = candidate.content.parts[0]\n",
    "        \n",
    "        # Check if it's a function call\n",
    "        if hasattr(first_part, 'function_call') and first_part.function_call:\n",
    "            function_call = first_part.function_call\n",
    "            \n",
    "            # Execute the bash command\n",
    "            try:\n",
    "                result = run_bash_command(**function_call.args)\n",
    "                \n",
    "                # removing TELEGRAM_BOT_TOKEN and TELEGRAM_USER_ID from print statement\n",
    "                TELEGRAM_BOT_TOKEN = os.environ.get(\"TELEGRAM_BOT_TOKEN\")\n",
    "                TELEGRAM_USER_ID = os.environ.get(\"TELEGRAM_USER_ID\")\n",
    "                command = function_call.args.get('command', 'unknown').replace(TELEGRAM_BOT_TOKEN, \"<telegram-token>\").replace(TELEGRAM_USER_ID, \"<telegram-user-id>\")\n",
    "                print(f\"[Executing: {command}]\")\n",
    "            except Exception as err:\n",
    "                result = f\"error: {err}\"\n",
    "            \n",
    "            # Add function call and result to history\n",
    "            history.append({\"role\": \"model\", \"parts\": [{\"function_call\": function_call}]})\n",
    "            history.append({\"role\": \"user\", \"parts\": [{\"function_response\": {\n",
    "                \"name\": function_call.name,\n",
    "                \"response\": {\"result\": result}\n",
    "            }}]})\n",
    "            \n",
    "            # Continue loop to allow more function calls\n",
    "            continue\n",
    "        \n",
    "        # Check if it's text - this means we're done\n",
    "        if hasattr(first_part, 'text'):\n",
    "            return response.text\n",
    "        \n",
    "        return \"I couldn't understand the response.\"\n",
    "    \n",
    "    return \"Maximum iterations reached. The assistant may be stuck in a loop.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa14388c",
   "metadata": {},
   "source": [
    "### 3.4 Configure the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e881932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = types.GenerateContentConfig(\n",
    "    tools=[tools],\n",
    "    system_instruction=f\"You are a helpful assistant that can execute bash commands. cwd: {os.getcwd()}. Read SKILLS.md and TASKS.md using `cat SKILLS.md` and `cat TASKS.md` before responding anything. You are autonomous agent, **Execute tasks fully without asking for follow up questions or statements**. e.g. never reply with: \\\"I will now send these to you on Telegram...\\\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07477fa5",
   "metadata": {},
   "source": [
    "### 3.5 Main Loop\n",
    "- query: search arxiv for papers on llm and send them to me on telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c5a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenClaw Assistant - Type 'exit' or 'quit' to end\n",
      "\n",
      "User: find research papers on llm from arxiv and send them to me on telegram\n",
      "[Executing: cat SKILLS.md]\n",
      "[Executing: cat TASKS.md]\n",
      "[Executing: curl \"https://export.arxiv.org/api/query?search_query=all:llm&start=0&max_results=3&sortBy=submittedDate&sortOrder=descending\"]\n",
      "[Executing: TELEGRAM_BOT_TOKEN=$(grep TELEGRAM_BOT_TOKEN .env | cut -d '=' -f2)\n",
      "TELEGRAM_USER_ID=$(grep TELEGRAM_USER_ID .env | cut -d '=' -f2)\n",
      "\n",
      "MESSAGE=\"Here are the latest research papers on LLM from arxiv:\n",
      "\n",
      "**1. AttentionRetriever: Attention Layers are Secretly Long Document Retrievers**\n",
      "*Summary:* Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.\n",
      "*Link:* https://arxiv.org/pdf/2602.12278v1\n",
      "\n",
      "**2. Agentic Test-Time Scaling for WebAgents**\n",
      "*Summary:* Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent's own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.\n",
      "*Link:* https://arxiv.org/pdf/2602.12276v1\n",
      "\n",
      "**3. CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use**\n",
      "*Summary:* AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2, an RL framework that replaces verifiable outcome rewards with checklist rewards. CM2 decomposes each turn's intended behavior into fine-grained binary criteria with explicit evidence grounding and structured metadata, turning open-ended judging into more stable classification-style decisions. To balance stability and informativeness, our method adopts a strategy of sparse reward assignment but dense evaluation criteria. Training is performed in a scalable LLM-simulated tool environment, avoiding heavy engineering for large tool sets. Experiments show that CM2 consistently improves over supervised fine-tuning. Starting from an 8B Base model and training on an 8k-example RL dataset, CM2 improves over the SFT counterpart by 8 points on tau^-Bench, by 10 points on BFCL-V4, and by 12 points on ToolSandbox. The results match or even outperform similarly sized open-source baselines, including the judging model. CM2 thus provides a scalable recipe for optimizing multi-turn, multi-step tool-using agents without relying on verifiable rewards. Code provided by the open-source community: https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent.\n",
      "*Link:* https://arxiv.org/pdf/2602.12268v1\"\n",
      "\n",
      "curl -s -X POST \"https://api.telegram.org/bot$TELEGRAM_BOT_TOKEN/sendMessage\" -d chat_id=\"$TELEGRAM_USER_ID\" -d text=\"$MESSAGE\" -d parse_mode=\"Markdown\"]\n",
      "[Executing: TELEGRAM_BOT_TOKEN=$(grep TELEGRAM_BOT_TOKEN .env | cut -d '=' -f2)\n",
      "TELEGRAM_USER_ID=$(grep TELEGRAM_USER_ID .env | cut -d '=' -f2)\n",
      "\n",
      "MESSAGE1=\"**1. AttentionRetriever: Attention Layers are Secretly Long Document Retrievers**\n",
      "*Summary:* Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.\n",
      "*Link:* https://arxiv.org/pdf/2602.12278v1\"\n",
      "\n",
      "curl -s -X POST \"https://api.telegram.org/bot$TELEGRAM_BOT_TOKEN/sendMessage\" -d chat_id=\"$TELEGRAM_USER_ID\" -d text=\"$MESSAGE1\" -d parse_mode=\"Markdown\"]\n",
      "[Executing: TELEGRAM_BOT_TOKEN=$(grep TELEGRAM_BOT_TOKEN .env | cut -d '=' -f2)\n",
      "TELEGRAM_USER_ID=$(grep TELEGRAM_USER_I .env | cut -d '=' -f2)\n",
      "\n",
      "MESSAGE2=\"**2. Agentic Test-Time Scaling for WebAgents**\n",
      "*Summary:* Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent's own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.\n",
      "*Link:* https://arxiv.org/pdf/2602.12276v1\"\n",
      "\n",
      "curl -s -X POST \"https://api.telegram.org/bot$TELEGRAM_BOT_TOKEN/sendMessage\" -d chat_id=\"$TELEGRAM_USER_ID\" -d text=\"$MESSAGE2\" -d parse_mode=\"Markdown\"]\n",
      "[Executing: TELEGRAM_BOT_TOKEN=$(grep TELEGRAM_BOT_TOKEN .env | cut -d '=' -f2)\n",
      "TELEGRAM_USER_ID=$(grep TELEGRAM_USER_ID .env | cut -d '=' -f2)\n",
      "\n",
      "MESSAGE3=\"**3. CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use**\n",
      "*Summary:* AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2, an RL framework that replaces verifiable outcome rewards with checklist rewards. CM2 decomposes each turn's intended behavior into fine-grained binary criteria with explicit evidence grounding and structured metadata, turning open-ended judging into more stable classification-style decisions. To balance stability and informativeness, our method adopts a strategy of sparse reward assignment but dense evaluation criteria. Training is performed in a scalable LLM-simulated tool environment, avoiding heavy engineering for large tool sets. Experiments show that CM2 consistently improves over supervised fine-tuning. Starting from an 8B Base model and training on an 8k-example RL dataset, CM2 improves over the SFT counterpart by 8 points on tau^-Bench, by 10 points on BFCL-V4, and by 12 points on ToolSandbox. The results match or even outperform similarly sized open-source baselines, including the judging model. CM2 thus provides a scalable recipe for optimizing multi-turn, multi-step tool-using agents without relying on verifiable rewards. Code provided by the open-source community: https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent.\n",
      "*Link:* https://arxiv.org/pdf/2602.12268v1\"\n",
      "\n",
      "curl -s -X POST \"https://api.telegram.org/bot$TELEGRAM_BOT_TOKEN/sendMessage\" -d chat_id=\"$TELEGRAM_USER_ID\" -d text=\"$MESSAGE3\" -d parse_mode=\"Markdown\"]\n",
      "Assistant: I have found the latest research papers on LLM from arxiv and sent them to you on Telegram.\n",
      "\n",
      "User: \n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "\n",
    "print(\"OpenClaw Assistant - Type 'exit' or 'quit' to end\\n\")\n",
    "\n",
    "while True:\n",
    "    query = input()\n",
    "    print(f\"User: {query}\")\n",
    "\n",
    "    if query.lower() in ['exit', 'quit', '']:\n",
    "        break\n",
    "    \n",
    "    # Add user query to history\n",
    "    history.append({\"role\": \"user\", \"parts\": [{\"text\": query}]})\n",
    "    \n",
    "    # Get response\n",
    "    response = respond(history, config)\n",
    "    print(f\"Assistant: {response}\\n\")\n",
    "    \n",
    "    # Add model response to history\n",
    "    history.append({\"role\": \"model\", \"parts\": [{\"text\": response}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0e2d0a",
   "metadata": {},
   "source": [
    "### 3.6 Heartbeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497f00ee",
   "metadata": {},
   "source": [
    "#### 3.6.1 TASKS.md\n",
    "- lets write `TASKS.md` to define things the agent should do when it wakes up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e6ad12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting TASKS.md\n"
     ]
    }
   ],
   "source": [
    "%%writefile TASKS.md\n",
    "find research papers on llm from arxiv and send them to me on telegram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be14ec6",
   "metadata": {},
   "source": [
    "### 3.6.2 Heartbeat Loop Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92a5508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Executing: cat SKILLS.md]\n",
      "[Executing: cat TASKS.md]\n",
      "[Executing: curl \"https://export.arxiv.org/api/query?search_query=all:llm&start=0&max_results=3&sortBy=submittedDate&sortOrder=descending\"]\n",
      "[Executing: cat .env]\n",
      "[Executing: \n",
      "curl -s -X POST \"https://api.telegram.org/bot<telegram-token>/sendMessage\" -d \"chat_id=<telegram-user-id>\" --data-urlencode \"text=Here are the latest research papers on LLMs:\n",
      "\n",
      "**1. AttentionRetriever: Attention Layers are Secretly Long Document Retrievers**\n",
      "*Link:* http://arxiv.org/abs/2602.12278v1\n",
      "*Summary:* This paper proposes AttentionRetriever, a novel long document retrieval model that leverages the attention mechanism and entity-based retrieval to build context-aware embeddings for long documents and determine the scope of retrieval.\n",
      "\n",
      "**2. Agentic Test-Time Scaling for WebAgents**\n",
      "*Link:* http://arxiv.org/abs/2602.12276v1\n",
      "*Summary:* This work introduces Confidence-Aware Test-Time Scaling (CATTS), a technique that uses vote-derived uncertainty to dynamically allocate compute for multi-step agents, improving performance and efficiency.\n",
      "\n",
      "**3. CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use**\n",
      "*Link:* http://arxiv.org/abs/2602.12268v1\n",
      "*Summary:* CM2 is an RL framework that uses checklist rewards to optimize multi-turn, multi-step tool-using agents without relying on verifiable rewards, showing consistent improvements over supervised fine-tuning.\"\n",
      "]\n",
      "Assistant: I have successfully executed the request. I found the latest research papers on LLMs from arxiv and sent them to the user on Telegram.\n",
      "\n",
      "sleeping...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "history = []\n",
    "\n",
    "HEARTBEAT_INTERVAL = 86400 # Run once every 24 hours\n",
    "while True:\n",
    "    # query = input(\"User:\")\n",
    "    query = \"Read TASKS.md, and execute the instructions\"\n",
    "    \n",
    "    if query.lower() in ['exit', 'quit', '']:\n",
    "        break\n",
    "    \n",
    "    # Add user query to history\n",
    "    history.append({\"role\": \"user\", \"parts\": [{\"text\": query}]})\n",
    "    \n",
    "    # Get response\n",
    "    response = respond(history, config)\n",
    "    print(f\"Assistant: {response}\\n\")\n",
    "    \n",
    "    # Add model response to history\n",
    "    history.append({\"role\": \"model\", \"parts\": [{\"text\": response}]})\n",
    "\n",
    "    # Note: This is the Only thing we are adding in previous code\n",
    "    print(\"sleeping...\")\n",
    "    time.sleep(HEARTBEAT_INTERVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9f1ec5",
   "metadata": {},
   "source": [
    "#### #### 3.6.3 Non-Blocking Heartbeat (Background Thread)\n",
    "\n",
    "```\n",
    "- Main loop → reactive user input\n",
    "- Background thread → periodic tasks (heartbeat)\n",
    "- Both run concurrently, independent of each other\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dac8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Executing: cat SKILLS.md]\n",
      "[Executing: cat TASKS.md]\n",
      "[Executing: curl \"https://export.arxiv.org/api/query?search_query=all:llm&start=0&max_results=3&sortBy=submittedDate&sortOrder=descending\"]\n",
      "User:  Add this to TASKS.md: find research papers on llm from arxiv and send them to me on telegram\n",
      "[Executing: echo \"find research papers on llm from arxiv and send them to me on telegram\" >> TASKS.md]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: I have added the task to `TASKS.md`. Now I will read the `SKILLS.md` and `TASKS.md` files to get a complete understanding of my capabilities and all assigned tasks.\n",
      "\n",
      "User:  \n",
      "[Executing: cat .env]\n",
      "[Executing: cat SKILLS.md]\n",
      "[Executing: cat TASKS.md]\n",
      "[Executing: curl \"https://export.arxiv.org/api/query?search_query=all:llm&start=0&max_results=3&sortBy=submittedDate&sortOrder=descending\"]\n",
      "[Executing: curl \"https://export.arxiv.org/api/query?search_query=all:llm&start=0&max_results=3&sortBy=submittedDate&sortOrder=descending\" > arxiv_papers.xml]\n",
      "[Executing: sudo apt-get install -y xml-twig-tools]\n",
      "[Executing: \n",
      "MESSAGE=\"**New Research Papers on LLM**\n",
      "\n",
      "**1. AttentionRetriever: Attention Layers are Secretly Long Document Retrievers**\n",
      "*Summary:* Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.\n",
      "\n",
      "**2. Agentic Test-Time Scaling for WebAgents**\n",
      "*Summary:* Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent's own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.\n",
      "\n",
      "**3. CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use**\n",
      "*Summary:* AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2, an RL framework that replaces verifiable outcome rewards with checklist rewards. CM2 decomposes each turn's intended behavior into fine-grained binary criteria with explicit evidence grounding and structured metadata, turning open-ended judging into more stable classification-style decisions. To balance stability and informativeness, our method adopts a strategy of sparse reward assignment but dense evaluation criteria. Training is performed in a scalable LLM-simulated tool environment, avoiding heavy engineering for large tool sets. Experiments show that CM2 consistently improves over supervised fine-tuning. Starting from an 8B Base model and training on an 8k-example RL dataset, CM2 improves over the SFT counterpart by 8 points on tau^-Bench, by 10 points on BFCL-V4, and by 12 points on ToolSandbox. The results match or even outperform similarly sized open-source baselines, including the judging model. CM2 thus provides a scalable recipe for optimizing multi-turn, multi-step tool-using agents without relying on verifiable rewards. Code provided by the open-source community: https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent.\n",
      "\"\n",
      "\n",
      "curl -X POST -H 'Content-Type: application/json' \\\n",
      "-d '{\"chat_id\": \"<telegram-user-id>\", \"text\": \"'\"${MESSAGE}\"'\", \"parse_mode\": \"Markdown\"}' \\\n",
      "https://api.telegram.org/bot<telegram-token>/sendMessage\n",
      "]\n",
      "[Executing: papers=$(grep -o '<title>.*</title>\\|<summary>.*</summary>\\|<link href=\"http[^\"]*\"/>' arxiv_papers.xml | sed 's/<[^>]*>//g' | sed 's/&lt;[^&gt;]*&gt;//g' | awk '{printf \"%s \", $0} END {print \"\"}' | sed 's/https/\\\\nhttps/g' | sed 's/Summary/\\\\nSummary/g' | sed 's/Title/\\\\nTitle/g') && echo \"$papers\"]\n",
      "[Executing: cat .env]\n",
      "[Executing: \n",
      "MESSAGE=\"**1. AttentionRetriever: Attention Layers are Secretly Long Document Retrievers**\n",
      "\n",
      "*Summary:* Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.\"\n",
      "\n",
      "curl -X POST -H 'Content-Type: application/json' \\\n",
      "-d '{\"chat_id\": \"<telegram-user-id>\", \"text\": \"'\"${MESSAGE}\"'\", \"parse_mode\": \"Markdown\"}' \\\n",
      "https://api.telegram.org/bot<telegram-token>/sendMessage\n",
      "]\n",
      "[Executing: papers=$(cat arxiv_papers.xml | grep -o '<entry>.*</entry>' | sed 's/<entry>/Title: /' | sed 's/<\\/entry>/\\\\n/' | sed 's/<[^>]*>//g' | sed 's/&lt;[^&gt;]*&gt;//g' | sed 's/http/\\\\nLink: http/g' | sed 's/Summary/\\\\nSummary/g')\n",
      "\n",
      "message=\"Here are the latest research papers on LLMs from arXiv:\n",
      "\n",
      "$papers\"\n",
      "\n",
      "TELEGRAM_BOT_TOKEN=$(cat .env | grep TELEGRAM_BOT_TOKEN | cut -d '=' -f2)\n",
      "TELEGRAM_USER_ID=$(cat .env | grep TELEGRAM_USER_ID | cut -d '=' -f2)\n",
      "\n",
      "curl -s -X POST \"https://api.telegram.org/bot$TELEGRAM_BOT_TOKEN/sendMessage\" -d chat_id=\"$TELEGRAM_USER_ID\" -d text=\"$message\"]\n",
      "Assistant: I have successfully sent the research papers to you on Telegram. Let me know if you have any other requests!\n",
      "\n",
      "User:  \n",
      "[Executing: cat SKILLS.md]\n",
      "[Executing: \n",
      "MESSAGE=\"**2. Agentic Test-Time Scaling for WebAgents**\n",
      "\n",
      "*Summary:* Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent's own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.\"\n",
      "\n",
      "curl -X POST -H 'Content-Type: application/json' \\\n",
      "-d '{\"chat_id\": \"<telegram-user-id>\", \"text\": \"'\"${MESSAGE}\"'\", \"parse_mode\": \"Markdown\"}' \\\n",
      "https://api.telegram.org/bot<telegram-token>/sendMessage\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "# Background Loop for TASKS.md\n",
    "# -----------------------------\n",
    "def heartbeat_loop(HEARTBEAT_INTERVAL):\n",
    "    while True:\n",
    "        \n",
    "        query = \"Read TASKS.md, and execute the instructions\"\n",
    "        response = respond([{\"role\": \"user\", \"parts\": [{\"text\": query}]}], config)\n",
    "        print(f\"Assistant (heartbeat): {response}\\n\")\n",
    "        time.sleep(HEARTBEAT_INTERVAL)\n",
    "\n",
    "# Run heartbeat in background\n",
    "threading.Thread(target=heartbeat_loop, args=(86400,), daemon=True).start()\n",
    "\n",
    "\n",
    "# Main reactive loop\n",
    "# -------------------\n",
    "while True:\n",
    "    query = input()\n",
    "    print(\"User: \", query)\n",
    "    if query.lower() in ['exit', 'quit']:\n",
    "        break\n",
    "    response = respond([{\"role\": \"user\", \"parts\": [{\"text\": query}]}])\n",
    "    print(f\"Assistant: {response}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
